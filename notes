Subject: experience replay:
  - Goal: 1. Compare the performance of difference types of experience replay within multiple environments:
         - Start with these types: prioritised experience replay and hindsight experience replay.Baseline: random shuffle of memory buffer.
           (HER=https://arxiv.org/pdf/1707.01495.pdf)(PER=https://arxiv.org/pdf/1511.05952.pdf)
         - Start with these environments: balancing a pole for now, put it within stijn's framework, start putting new environments into it.
         Why is this interesting? 
         - Experience replay is commonly used to avoid 'forgetting' old experiences and prioritizing only the new ones. Essentially, experiences
           are combined into one big bucket and shuffled up when training. A buffer of past experiences is used to stabilize training by 
           decorrelating the training examples in each batch used to update the neural network
         - Hindsight experience replay allows us to 'imagine' experiences and use those to train our model. It turns out that this is very helpful
           especially in situations where our tasks reward is very sparse, for example the mountair car game. 
